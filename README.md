# Sprint 2: Extensible CSV

### User Story 1

- #### Reflect. How do you evaluate correctness when you don’t understand the artifact fully? What do you believe you understand (or not)?
  I used tests to compare regex behavior. Even if I don’t fully grasp it, if a regex passes tests for quoted fields, commas inside quotes, escaped quotes, etc, then I can be confident it’s “good enough.” I don’t think it’s ever “100% correct,” but the tests build trust. I guess what I don’t fully understand are deeper edge cases like newlines inside quotes.

- #### Implementation Choice: Between the four regexes we provided and the one generated by the LLM, which do you believe is the best, and therefore have chosen to implement? Why?
  I chose regex3. It handled quoted commas and internal quotes more consistently than regex1/2, and didn’t break as much as regex4. The LLM regex failed some of my tests. Regex3 isn’t perfect, but from my tests it gave the most reliable results.

---

### User Story 4

- #### Document your choice and justify your reasoning for the information you include and exclude.
  I created a `CSVError` class. It includes the row index and Zod error issues. I excluded extra info like raw line text to keep errors focused and not overload the developer. The important part is knowing what row failed and why.

- #### You’re building a developer-facing tool, and your user has provided the schema, so they’re in the best position to fix or handle bad data. But what happens after an error? Should your parser stop immediately when it hits a bad row, or keep going and continue yielding rows even after an error? Document your choice and justify your reasoning.
  My parser stops immediately. I did this so the caller knows right away if something is wrong. It’s safer because otherwise you could end up working with bad data downstream without realizing it. Caller can always catch the error and decide what to do.

---

### User Story 6
I added stricter empty cell handling (rows with trailing empty values are trimmed). This makes parsing cleaner and avoids false “blank” cells at the end. This was part of my Sprint 1 enhancement list.

---

### Reflection

- 1. #### Different user types
   Writing for other developers means I can’t assume they know my hacks or shortcuts. I had to think about what info they need (headers, errors, structured objects) and make sure it’s obvious and consistent. Technical empathy = build it like I’d want to use it.

---

### Design Choices

#### Errors/Bugs:
- Regexes still aren’t perfect — LLM regex failed quoted commas/escaped quotes tests.
- Bob’s row fails schema because “thirty” isn’t a number. This is expected though and shows validation works.

#### Tests:
- Regex tests comparing all 4 provided regexes, plus LLM regex.
- Tests for flaws in each regex (like quoted commas, escaped quotes).
- Parser tests: headers vs no headers, empty cells, same column count, schema validation, generator rows.

---

#### Collaborators (cslogins of anyone you worked with on this project or generative AI):
- Used generative AI for debugging + helped me with reflections

#### Total estimated time it took to complete project:
5 hours

#### Link to GitHub Repo:
https://github.com/cs0320-f25/extensible-csv-ghassanjomaa